# Task 1: Text Preprocessing Operations

## Step 1: Created `clean_text` Function for Text Preprocessing
Developed a function named `clean_text` to perform:
- Lowercasing of text
- Removal of punctuation
- Elimination of numerical expressions

## Step 2: Applied the Function to All Texts in the Dataset
Applied the `clean_text` function to process all texts within the dataset.

## Step 3: Created `remove_stopwords` Function to Remove Unimportant Words During Feature Extraction
Implemented a function named `remove_stopwords` to remove non-essential words from the text.

# Task 2: Writing All Steps as a Single Function

## Step 1: Conducted Text Preprocessing Operations
Performed text preprocessing tasks such as lowercasing, punctuation removal, and numerical expression elimination.

## Step 2: Integrated Visualization Operations as Arguments into the Function
Added visualization operations as arguments to the function.

## Step 3: Documented the Function with a Docstring
Wrote a detailed docstring explaining the function's purpose, steps, and parameters.

## Step 4: Applied the Combined Function to All Texts in the Dataset
Executed the combined function across all texts in the dataset.

## Step 5: Identified and Removed Less Frequently Occurring Words (e.g., fewer than 1000 occurrences)
Identified words in the text dataset that appear infrequently (e.g., less than 1000 occurrences) and removed them.

## Step 6: Tokenized the Texts
Tokenized the processed texts into individual tokens (words).

## Step 7: Performed Lemmatization
Applied lemmatization to reduce words to their base forms.

# Task 3: Data Visualization

## Step 1: Calculated Term Frequencies in the Texts
Computed the frequencies of terms (words) in the texts.

## Step 2: Created a Barplot of Term Frequencies
Generated a barplot to visualize the frequencies of terms identified in the previous step.

## Step 3: Visualized Words Using WordCloud
Created a WordCloud to visually represent the words based on their frequencies.
